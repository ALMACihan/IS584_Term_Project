{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyNWqzWHeSUOqyzl64Awx0Ic",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ALMACihan/IS584_Term_Project/blob/main/Project_Cihan_Alma.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " **IS 584 Term Project - Cihan Alma**"
      ],
      "metadata": {
        "id": "emXyCqi_pv1c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q \"transformers==4.52.3\" \"datasets>=2.0.0\" \"scikit-learn>=1.3.0\" \"wandb>=0.16.0\"\n",
        "\n",
        "\n",
        "import wandb\n",
        "import pandas as pd\n",
        "from datasets import load_dataset\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "FXP-hbMhnFRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q scikit-learn pandas\n"
      ],
      "metadata": {
        "id": "hnElJ5CLpvWY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import csv\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"/content/asap_review_dataset.csv\",\n",
        "    on_bad_lines='skip',        # Replaces error_bad_lines\n",
        "    quoting=csv.QUOTE_ALL,      # Handle quoted fields\n",
        "    encoding=\"utf-8\",\n",
        "    engine=\"python\"\n",
        ")\n",
        "\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "kANgvdahpwVE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional: filter to most common aspect labels (optional cleanup)\n",
        "print(df['aspect_label'].value_counts())\n",
        "\n",
        "# Drop duplicates\n",
        "df = df.drop_duplicates(subset=[\"paper_id\", \"aspect_label\", \"review_text\"])\n"
      ],
      "metadata": {
        "id": "lSqwCk8hpyh5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df[\"review_text\"], df[\"aspect_label\"], test_size=0.2, random_state=42, stratify=df[\"aspect_label\"]\n",
        ")\n",
        "\n",
        "# Vectorize\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Train\n",
        "clf = LogisticRegression(max_iter=1000, class_weight=\"balanced\")\n",
        "clf.fit(X_train_vec, y_train)\n",
        "\n",
        "# Evaluate\n",
        "y_pred = clf.predict(X_test_vec)\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "TG9DYhCxp1Lq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample ~4k examples evenly from all labels\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"/content/asap_review_dataset.csv\",\n",
        "    on_bad_lines='skip',        # Replaces error_bad_lines\n",
        "    quoting=csv.QUOTE_ALL,      # Handle quoted fields\n",
        "    encoding=\"utf-8\",\n",
        "    engine=\"python\"\n",
        ")\n",
        "\n",
        "df_sampled = df.groupby(\"aspect_label\").apply(lambda x: x.sample(n=min(len(x), 300), random_state=42)).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "22m6P7IcAjXQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import classification_report\n",
        "from datasets import Dataset\n",
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import wandb\n",
        "\n",
        "wandb.login()\n"
      ],
      "metadata": {
        "id": "5uDLo4kQwGkp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CSV\n",
        "df = pd.read_csv(\n",
        "    \"/content/asap_review_dataset.csv\",\n",
        "    on_bad_lines='skip',        # Replaces error_bad_lines\n",
        "    quoting=csv.QUOTE_ALL,      # Handle quoted fields\n",
        "    encoding=\"utf-8\",\n",
        "    engine=\"python\"\n",
        ")\n",
        "df = df.dropna(subset=[\"review_text\", \"aspect_label\"])\n",
        "df = df[df[\"review_text\"].str.strip().astype(bool)]\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[\"aspect_label\"])\n",
        "\n",
        "df = df.groupby(\"aspect_label\").apply(lambda x: x.sample(n=min(len(x), 200), random_state=42)).reset_index(drop=True)\n",
        "\n",
        "\n",
        "# Create HuggingFace Dataset\n",
        "dataset = Dataset.from_pandas(df[[\"review_text\", \"label\"]])\n",
        "dataset = dataset.train_test_split(test_size=0.2, seed=42)\n",
        "\n",
        "label2id = {label: i for i, label in enumerate(le.classes_)}\n",
        "id2label = {i: label for label, i in label2id.items()}\n",
        "num_labels = len(label2id)\n"
      ],
      "metadata": {
        "id": "oGK9yuskwHb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(example[\"review_text\"], padding=\"max_length\", truncation=True, max_length=512)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, batched=True)\n"
      ],
      "metadata": {
        "id": "C-I65Cc1wNSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "0kADaZ-TwOec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    report = classification_report(labels, preds, output_dict=True, zero_division=0)\n",
        "    return {\n",
        "        \"accuracy\": report[\"accuracy\"],\n",
        "        \"macro_f1\": report[\"macro avg\"][\"f1-score\"]\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "zAf-NgbBwP0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate()\n",
        "print(\"Final Eval Results:\", results)\n"
      ],
      "metadata": {
        "id": "XptKa3y2wR8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Second Training\n",
        "learning_rate=5e-5,\n",
        "batch_size=16,\n",
        "\n"
      ],
      "metadata": {
        "id": "I9px4ucCFSpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nPcgn-KCGLNw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "id": "qllBSHBRFVFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    report = classification_report(labels, preds, output_dict=True, zero_division=0)\n",
        "    return {\n",
        "        \"accuracy\": report[\"accuracy\"],\n",
        "        \"macro_f1\": report[\"macro avg\"][\"f1-score\"]\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "_PGHg4WoFfn8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = trainer.evaluate()\n",
        "print(\"Final Eval Results:\", results)\n"
      ],
      "metadata": {
        "id": "PhgUcwTOFiWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_baseline\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "wYDG47X7GbNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    report = classification_report(labels, preds, output_dict=True, zero_division=0)\n",
        "    return {\n",
        "        \"accuracy\": report[\"accuracy\"],\n",
        "        \"macro_f1\": report[\"macro avg\"][\"f1-score\"]\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "WNY6d6GYG-IS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_batch16\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WXKWFCNXG_-E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    report = classification_report(labels, preds, output_dict=True, zero_division=0)\n",
        "    return {\n",
        "        \"accuracy\": report[\"accuracy\"],\n",
        "        \"macro_f1\": report[\"macro avg\"][\"f1-score\"]\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "eTBQVYbrHWNz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\",\n",
        "    num_labels=num_labels,\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ")\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results_warmup_wd\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    logging_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=10,\n",
        "    weight_decay=0.05,           # new\n",
        "    warmup_steps=500,            # new\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"accuracy\",\n",
        "    report_to=\"wandb\"\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sdbd9MzvHZ9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    preds = np.argmax(predictions, axis=1)\n",
        "    report = classification_report(labels, preds, output_dict=True, zero_division=0)\n",
        "    return {\n",
        "        \"accuracy\": report[\"accuracy\"],\n",
        "        \"macro_f1\": report[\"macro avg\"][\"f1-score\"]\n",
        "    }\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"test\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "iJZb8jl8Hfuk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}